#druid
#spring.datasource.druid.url=jdbc:mysql://localhost:3306/kafka_test?useUnicode=true&characterEncoding=UTF-8
#spring.datasource.druid.username=root
#spring.datasource.druid.password=123456789
#public-key=MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBAIaZeUjq4jOXEwL/HyxhsAGwOZq4PA2ctfLPONfvRQKpOqeX7upvSmxJ3cr/paScbVaTldaPOjiQUJr2GiAx3KUCAwEAAQ==
#spring.datasource.druid.connectionProperties=config.decrypt=true;config.decrypt.key=${public-key}
#spring.datasource.druid.filters=config,stat

#spring.datasource.druid.initial-size=1
#spring.datasource.druid.max-active=20
#spring.datasource.druid.min-idle=1
#spring.datasource.druid.max-wait=60000
#spring.datasource.druid.pool-prepared-statements=true
#spring.datasource.druid.max-pool-prepared-statement-per-connection-size=20
#spring.datasource.druid.validation-query=SELECT 1 FROM DUAL
#spring.datasource.druid.test-on-borrow=false
#spring.datasource.druid.test-on-return=false
#spring.datasource.druid.test-while-idle=true
#spring.datasource.druid.time-between-eviction-runs-millis=60000
#spring.datasource.druid.min-evictable-idle-time-millis=300000

#spring.datasource.druid.filter.slf4j.enabled=true
#spring.datasource.druid.filter.slf4j.statement-executable-sql-log-enable=true

spring.datasource.url=jdbc:mysql://localhost:3306/kafka_test?useUnicode=true&characterEncoding=UTF-8
spring.datasource.username=root
spring.datasource.password=123456789
spring.datasource.validation-query=SELECT 1 FROM DUAL
spring.datasource.test-on-borrow=true

server.port=8082
server.servlet.context-path=/hello

spring.servlet.multipart.max-file-size=50Mb
spring.servlet.multipart.max-request-size=50Mb

#redis
spring.redis.host=192.168.10.31
spring.redis.port=6379

#spring.http.encoding.force=true
#spring.http.encoding.charset=UTF-8
#spring.http.encoding.enabled=true
#server.tomcat.uri-encoding=UTF-8

#spring.mvc.view.prefix=/pages/
#spring.mvc.view.suffix=.html

#web.upload-path=/Users/king/qmth/work_text
#spring.mvc.static-path-pattern=/**
#spring.resources.static-locations=classpath:/META-INF/resources/,classpath:/resources/,classpath:/static/,classpath:/public/,file:${web.upload-path}
#mybatis
mybatis.mapper-locations=classpath:mybatis/mapper/*.xml
mybatis.type-aliases-package=distributed.transaction.model

#kafka
kafka.consumer.zookeeper.connect=localhost:2181
kafka.consumer.servers=localhost:9092
#enable.auto.commit
#offset偏移量自动提交，false为手动提交，默认为true
kafka.consumer.enable.auto.commit=true
#consumer.session.timeout 过期时间,默认值是：3000 （3s）
#heartbeat.interval.ms 消费信息心跳，数据量较大时使用
kafka.consumer.session.timeout=6000
#自动提交间隔,默认值是 5000 （5 s）
kafka.consumer.auto.commit.interval=100
#kafka.consumer.auto.offset.reset start
#这个配置项，是告诉Kafka Broker在发现kafka在没有初始offset，或者当前的offset是一个不存在的值（如果一个record被删除，就肯定不存在了）时，该如何处理。它有4种处理方式：
#1.earliest：自动重置到最早的offset。
#2.latest：看上去重置到最晚的offset。
#3.none：如果边更早的offset也没有的话，就抛出异常给consumer，告诉consumer在整个consumer group中都没有发现有这样的offset。
#4.如果不是上述3种，只抛出异常给consumer。
#kafka.consumer.auto.offset.reset end
kafka.consumer.auto.offset.reset=latest
kafka.consumer.topic=wangliang_test1
#用于表示该consumer想要加入到哪个group中
kafka.consumer.group.id=wangliang_test1
#消费者容器并发数
kafka.consumer.concurrency=10

kafka.producer.servers=localhost:9092
#acks=0： 设置为0表示producer不需要等待任何确认收到的信息。副本将立即加到socket buffer并认为已经发送。没有任何保障可以保证此种情况下server已经成功接收数据，同时重试配置不会发生作用
#acks=1： 这意味着至少要等待leader已经成功将数据写入本地log，但是并没有等待所有follower是否成功写入。这种情况下，如果follower没有成功备份数据，而此时leader又挂掉，则消息会丢失。
#acks=-1/all： 这意味着leader需要等待所有备份都成功写入日志，这种策略会保证只要有一个备份存活就不会丢失数据。这是最强的保证。
kafka.producer.acks=-1
#retries设置大于0的值将使客户端重新发送任何数据，一旦这些数据发送失败。注意，这些重试与客户端接收到发送错误时的重试没有什么不同。
kafka.producer.retries=0
#生产者将试图批处理消息记录，以减少请求次数。这将改善client与server之间的性能。这项配置控制默认的批量处理消息字节数
kafka.producer.batch.size=4096
#生产者将会汇总任何在请求与发送之间到达的消息记录一个单独批量的请求。通常来说，这只有在记录产生速度大于发送速度的时候才能发生。
kafka.producer.linger=1
#生产者可以用来缓存数据的内存大小。如果数据产生速度大于向broker发送的速度，producer会阻塞或者抛出异常，以“block.on.buffer.full”来表明
kafka.producer.buffer.memory=40960